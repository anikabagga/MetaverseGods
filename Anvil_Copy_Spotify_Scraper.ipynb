{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Anvil Copy Spotify Scraper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Last.fm ](https://www.last.fm/login?next=/api/account/create)\n",
        "\n",
        "* API: 912b321d8a267e7892932b6deccb3d2c"
      ],
      "metadata": {
        "id": "c91EH-HpMX8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes for how it would work:\n",
        "* Users need to create a Last.fm account and connect to Spotify "
      ],
      "metadata": {
        "id": "HUyMESIK_pYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pylast\n",
        "!pip install requests-toolbelt\n",
        "!pip install anvil-uplink\n",
        "\n",
        "import pylast\n",
        "import sys\n",
        "import pandas as pd\n",
        "import requests\n",
        "import pprint\n",
        "from requests_toolbelt.threaded import pool"
      ],
      "metadata": {
        "id": "jX2EdHmUY-GB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c5c2558-6167-4cdb-a8b9-13921d298948"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pylast\n",
            "  Downloading pylast-5.0.0-py3-none-any.whl (25 kB)\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.22.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 951 kB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from pylast) (4.11.3)\n",
            "Collecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx->pylast) (2021.10.8)\n",
            "Collecting httpcore<0.15.0,>=0.14.5\n",
            "  Downloading httpcore-0.14.7-py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.7/dist-packages (from httpx->pylast) (2.0.12)\n",
            "Collecting h11<0.13,>=0.11\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting anyio==3.*\n",
            "  Downloading anyio-3.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio==3.*->httpcore<0.15.0,>=0.14.5->httpx->pylast) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from anyio==3.*->httpcore<0.15.0,>=0.14.5->httpx->pylast) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->pylast) (3.8.0)\n",
            "Installing collected packages: sniffio, rfc3986, h11, anyio, httpcore, httpx, pylast\n",
            "Successfully installed anyio-3.5.0 h11-0.12.0 httpcore-0.14.7 httpx-0.22.0 pylast-5.0.0 rfc3986-1.5.0 sniffio-1.2.0\n",
            "Collecting requests-toolbelt\n",
            "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from requests-toolbelt) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt) (2021.10.8)\n",
            "Installing collected packages: requests-toolbelt\n",
            "Successfully installed requests-toolbelt-0.9.1\n",
            "Collecting anvil-uplink\n",
            "  Downloading anvil_uplink-0.3.42-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from anvil-uplink) (1.15.0)\n",
            "Collecting ws4py\n",
            "  Downloading ws4py-0.5.1.tar.gz (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 178 kB/s \n",
            "\u001b[?25hCollecting argparse\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from anvil-uplink) (0.16.0)\n",
            "Building wheels for collected packages: ws4py\n",
            "  Building wheel for ws4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ws4py: filename=ws4py-0.5.1-py3-none-any.whl size=45229 sha256=cf8bcfe462a6c2a8d790f8b172c8565cb84b75184f753f8da4b1f738f3b3116c\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/ea/7d/3410aa0aa0e4402ead9a7a97ab2214804887e0f5c2b76f0c96\n",
            "Successfully built ws4py\n",
            "Installing collected packages: ws4py, argparse, anvil-uplink\n",
            "Successfully installed anvil-uplink-0.3.42 argparse-1.4.0 ws4py-0.5.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse",
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pylast\n",
        "import sys\n",
        "import pandas as pd\n",
        "import requests\n",
        "import pprint\n",
        "from requests_toolbelt.threaded import pool\n",
        "\n",
        "import anvil.server\n",
        "from tables import app_tables\n",
        "\n",
        "LASTFM_API_KEY = '912b321d8a267e7892932b6deccb3d2c'\n",
        "#LASTFM_USER_NAME = 'metaversegods'\n",
        "TEXT = '#text'\n",
        "ESTIMATED_TIME_FOR_PROCESSING_PAGE = 352\n",
        "ESTIMATED_TIME_FOR_PROCESSING_DATAFRAME_PER_PAGE_OF_RESULTS = 275\n",
        "anvil.server.connect(\"PDXXW3H2G4I5G3H23UHPXT5N-5XLB6PIE4RL7PMCQ\")\n",
        "\n",
        "@anvil.server.callable\n",
        "def get_scrobbles(LASTFM_USER_NAME):\n",
        "  endpoint = \"recenttracks\"\n",
        "  username = LASTFM_USER_NAME\n",
        "  key = '912b321d8a267e7892932b6deccb3d2c'\n",
        "  limit = 200\n",
        "  extended = 0\n",
        "  page = 1\n",
        "  pages = 0\n",
        "\n",
        "  \"\"\"\n",
        "  endpoint: API endpoint.\n",
        "  username: Last.fm username to fetch scrobbles for.\n",
        "  key: API key.\n",
        "  limit: The number of records per page. Maximum is 200.\n",
        "  extended: Extended results from API, such as whether user has \"liked\" the track.\n",
        "  page: First page to retrieve.\n",
        "  pages: How many pages of results after \"page\" argument to retrieve. If 0, get all pages.\n",
        "  \"\"\"\n",
        "  # initialize URL and lists to contain response fields\n",
        "  url = (\n",
        "      \"https://ws.audioscrobbler.com/2.0/?method=user.get{}\"\n",
        "      \"&user={}\"\n",
        "      \"&api_key={}\"\n",
        "      \"&limit={}\"\n",
        "      \"&extended={}\"\n",
        "      \"&page={}\"\n",
        "      \"&format=json\")\n",
        "\n",
        "  # get total number of pages\n",
        "  request_url = url.format(endpoint, username, key, limit, extended, page)\n",
        "  response = requests.get(request_url).json()\n",
        "  total_pages = int(response[endpoint][\"@attr\"][\"totalPages\"])\n",
        "  if pages > 0:\n",
        "      total_pages = min([total_pages, pages])\n",
        "\n",
        "  print(\n",
        "      \"Total pages to retrieve: {}. Estimated time: {}\".format(\n",
        "          total_pages, get_time_remaining(total_pages)\n",
        "      )\n",
        "  )\n",
        "\n",
        "  artist_names = []\n",
        "  album_names = []\n",
        "  track_names = []\n",
        "  timestamps = []\n",
        "  urls = []\n",
        "\n",
        "  # add formatted URLs to list to be requested in thread pool\n",
        "  for page in range(0, int(total_pages) + 1, 1):\n",
        "      urls.append(url.format(endpoint, username, key, limit, extended, page))\n",
        "  p = pool.Pool.from_urls(urls)\n",
        "  p.join_all()\n",
        "\n",
        "  for response in p.responses():\n",
        "      if endpoint in response.json():\n",
        "          response_json = response.json()[endpoint][\"track\"]\n",
        "          for track in response_json:\n",
        "              if \"@attr\" not in track:\n",
        "                  artist_names.append(track[\"artist\"][TEXT])\n",
        "                  album_names.append(track[\"album\"][TEXT])\n",
        "                  track_names.append(track[\"name\"])\n",
        "                  timestamps.append(track[\"date\"][\"uts\"])\n",
        "\n",
        "  # create and populate a dataframe to contain the data\n",
        "  df = pd.DataFrame()\n",
        "  df[\"artist\"] = artist_names\n",
        "  df[\"album\"] = album_names\n",
        "  df[\"track\"] = track_names\n",
        "  df[\"timestamps\"] = timestamps\n",
        "\n",
        "  df[\"datetime\"] = pd.to_datetime(timestamps, unit=\"s\")\n",
        "  df[\"username\"] = username\n",
        "\n",
        "  df.sort_values(\"timestamps\", ascending=False, inplace=True)\n",
        "\n",
        "  list1 = df[[\"artist\",\"username\"]]\n",
        "  list1[\"playcount\"] = 1\n",
        "  list1 = list1.groupby([\"artist\",\"username\"]).count()\n",
        "  list1 = list1.sort_values(\"playcount\",ascending=False)\n",
        "  list1 = list1.reset_index()\n",
        "\n",
        "  for d in list1.to_dict(orient=\"records\"):\n",
        "      app_tables.tablee.add_row(**d)\n",
        "\n",
        "\n",
        "def get_time_remaining(pages_remaining):\n",
        "    \"\"\"Calculate the estimated time remaining.\"\"\"\n",
        "    millis_remaining = int(\n",
        "        (pages_remaining * ESTIMATED_TIME_FOR_PROCESSING_PAGE)\n",
        "        + (\n",
        "            pages_remaining\n",
        "            * ESTIMATED_TIME_FOR_PROCESSING_DATAFRAME_PER_PAGE_OF_RESULTS\n",
        "        )\n",
        "    )\n",
        "    seconds_remaining = (millis_remaining / 1000) % 60\n",
        "    seconds_remaining = int(seconds_remaining)\n",
        "    minutes_remaining = (millis_remaining / (1000 * 60)) % 60\n",
        "    minutes_remaining = int(minutes_remaining)\n",
        "    return \"{}m{:2}s\".format(minutes_remaining, seconds_remaining)"
      ],
      "metadata": {
        "id": "WuO6-6Sd_QS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcec5761-c7dc-43f5-c12f-4f6adc6c271d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pages to retrieve: 3. Estimated time: 0m 1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    }
  ]
}